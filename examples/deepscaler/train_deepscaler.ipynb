{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[WIP] Reproduction of [Deepscaler](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2) with Single-turn Agentic framework.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ih2uy0ZkJZa"
      }
    },
    {
      "metadata": {
        "id": "QNDch_ZJV3Xb"
      },
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import functools\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "\n",
        "from etils import ecolab\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from GOOGLE_INTERNAL_PACKAGE_PATH.perftools.accelerators.xprof.api.python import xprof_session\n",
        "from GOOGLE_INTERNAL_PACKAGE_PATH.pyglib import gfile\n",
        "from etils import ecolab\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "try:\n",
        "  from etils import ecolab\n",
        "  cm = ecolab.adhoc(\n",
        "      source=ecolab.FROM_NOTEBOOK_OR_HEAD,\n",
        "      reload='tunix',\n",
        "      behavior='preferred',\n",
        "      cell_autoreload=True,\n",
        "  )\n",
        "except:\n",
        "  import contextlib\n",
        "  cm = contextlib.nullcontext()\n",
        "\n",
        "with cm:\n",
        "  from tunix.models.qwen2 import params as params_lib\n",
        "  from tunix.models.qwen2 import model as model_lib\n",
        "  from tunix.generate import sampler as sampler_lib\n",
        "  from tunix.sft import metrics_logger\n",
        "  from tunix.rl.agentic.agents import model_agent\n",
        "  from tunix.rl.agentic.environments import task_environment\n",
        "  from tunix.rl.agentic.rewards import reward\n",
        "  from tunix.rl.agentic.trajectory import trajectory_collect_engine\n",
        "  from tunix.rl.agentic.parser.chat_template_parser import parser\n",
        "  from flax import nnx\n",
        "  import jax\n",
        "  import numpy as np\n",
        "  from tunix.rl.experimental.agentic_grpo_learner import GRPOConfig, GRPOLearner\n",
        "  from tunix.models.qwen2 import params\n",
        "  from tunix.models.qwen2 import model\n",
        "  from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "  from tunix.sft import utils\n",
        "  from tunix.rl.rollout import base_rollout\n",
        "  from tunix.sft import metrics_logger\n",
        "  from tunix.google.examples.deepscaler import math_rewards\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "0f3pplToi7Yi"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no means a \"perfect\" set of hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "nQ5mILtDi-IJ"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# ====== Data ======\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== Base Model ======\n",
        "NNX_CKPT_DIR = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/gemma2/nnx/\"\n",
        "MODEL_VERSION = \"2b-it\"\n",
        "\n",
        "# ====== Reproducibility ======\n",
        "SEED = 42\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 1024\n",
        "TOTAL_GENERATION_STEPS = 4096\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.5\n",
        "TOP_P = 0.95\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 1\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "# AIME only has 30 questions. we could repeat it after the demo can run.\n",
        "BATCH_SIZE = 1\n",
        "NUM_BATCHES = 30\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 50\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 1000  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9  # Adam beta1\n",
        "B2 = 0.99  # Adam beta2\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# ====== Checkpoint saving ======\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "DO_MEM_PROFILING = False\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}\n",
        "# ====== Rollout ======\n",
        "ROLLOUT_ENGINE = \"vanilla\"\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from GOOGLE_INTERNAL_PACKAGE_PATH.pyglib import gfile\n",
        "  file_open = gfile.Open\n",
        "\n",
        "  NOTEBOOK_ENV = \"g3\"\n",
        "except Exception:\n",
        "  NOTEBOOK_ENV = \"git\"\n",
        "\n",
        "  from google.cloud import storage\n",
        "\n",
        "  file_open = fsspec.open\n",
        "\n",
        "if NOTEBOOK_ENV == \"g3\":\n",
        "  DATA_PATH_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/rl/data/\"\n",
        "  MODEL_PATH_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/\"\n",
        "  CKPT_DIR_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/\"\n",
        "else:\n",
        "  DATA_PATH_PREFIX = \"gs://tunix/rl/data\"\n",
        "  MODEL_PATH_PREFIX = \"gs://tunix/rl/models\"\n",
        "  CKPT_DIR_PREFIX = \"gs://tunix/rl/checkpoints\"\n",
        "\n",
        "print(\"NOTEBOOK_ENV: \", NOTEBOOK_ENV)\n",
        "CKPT_DIR = os.path.join(CKPT_DIR_PREFIX, \"deepscaler_ckpt/01\")\n",
        "\n",
        "MODEL_VERSION = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "MODEL_PATH = os.path.join(MODEL_PATH_PREFIX, \"DeepSeek-R1-Distill-Qwen-1.5B\")"
      ],
      "metadata": {
        "id": "S7vLG_wtktdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_hbm_usage = rl_utils.show_hbm_usage"
      ],
      "metadata": {
        "id": "r4y0AT_UlWep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgscGppxkBm1"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "DEEPSCALER_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"DeepScaleR-Preview-Dataset/deepscaler.json\")\n",
        "AIME_2024_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"HuggingFaceH4/aime_2024/train-00000-of-00001.parquet\")\n",
        "\n",
        "def create_datasets(\n",
        "    train_ds_path: str = DEEPSCALER_DATA_PATH,\n",
        "    test_ds_path: str = AIME_2024_DATA_PATH\n",
        "):\n",
        "  def preprocess_fn(example, index):\n",
        "    return {\n",
        "        \"question\": example[\"problem\"],\n",
        "        \"ground_truth\": example[\"answer\"],\n",
        "        \"data_source\": \"math\",\n",
        "    }\n",
        "\n",
        "  with file_open(train_ds_path) as train_f, file_open(test_ds_path, 'rb') as test_f:\n",
        "    train_df = pd.read_json(train_f)\n",
        "    test_df = pd.read_parquet(test_f)\n",
        "\n",
        "  train_ds = Dataset.from_pandas(train_df).map(preprocess_fn, with_indices=True)\n",
        "  test_ds = Dataset.from_pandas(test_df).map(preprocess_fn, with_indices=True)\n",
        "\n",
        "\n",
        "  def process_item(item):\n",
        "      question = item[\"question\"]\n",
        "      answer = item[\"answer\"]\n",
        "\n",
        "      instruction = \"Let's think step by step, and put your final answer within \\\\boxed{}.\"\n",
        "      prompt = f\"{question} {instruction}\"\n",
        "      prompt = tokenizer.apply_chat_template(\n",
        "          [{\"role\": \"user\", \"content\": prompt}],\n",
        "          tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "      return {\n",
        "          \"prompts\": prompt,\n",
        "          \"question\": question,\n",
        "          \"answer\": answer,\n",
        "      }\n",
        "\n",
        "  train_ds = grain.MapDataset.source(train_ds).map(process_item)\n",
        "  test_ds = grain.MapDataset.source(test_ds).map(process_item)\n",
        "  return train_ds, test_ds"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_source = MODEL_PATH if NOTEBOOK_ENV == \"g3\" else MODEL_VERSION\n",
        "print(tokenizer_source)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)\n",
        "\n",
        "chat_parser = parser.QwenChatTemplateParser(tokenizer)\n"
      ],
      "metadata": {
        "id": "tsM5VlCqZYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset, test_dataset = create_datasets()\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = train_dataset[: int(len(train_dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = train_dataset[int(len(train_dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
        "\n",
        "for s in iter(train_dataset):\n",
        "  print(s)\n",
        "  break\n",
        "\n",
        "for s in iter(test_dataset):\n",
        "  print(s)\n",
        "  break"
      ],
      "metadata": {
        "id": "26fIP9ONl8HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_hbm_usage()"
      ],
      "metadata": {
        "id": "KqVQlqvklbZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mesh = jax.make_mesh((1, 4), ('fsdp', 'tp'))\n",
        "config = model_lib.ModelConfig.deepseek_r1_distill_qwen_1_5b()\n",
        "qwen2 = params_lib.create_model_from_safe_tensors(MODEL_PATH, config, mesh, dtype=jnp.float32)\n",
        "# nnx.display(model)"
      ],
      "metadata": {
        "id": "yuUrHjYxkmbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_hbm_usage()"
      ],
      "metadata": {
        "id": "ViHYfWhvlYrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ModelAgent = model_agent.ModelAgent\n",
        "TaskEnvironment = task_environment.TaskEnvironment\n",
        "TrajectoryCollectEngine = trajectory_collect_engine.TrajectoryCollectEngine\n",
        "is_two_reward = reward.is_two_reward"
      ],
      "metadata": {
        "id": "Q6IhdLZgnErz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ],
      "metadata": {
        "id": "CBgLgxWJnTBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ],
      "metadata": {
        "id": "-m3qN7gmnQpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs\n",
        "if NOTEBOOK_ENV == \"g3\":\n",
        "  %load_ext GOOGLE_INTERNAL_PACKAGE_PATH.learning.brain.tensorboard.notebook.extension\n",
        "else:\n",
        "  %load_ext tensorboard\n",
        "%tensorboard --logdir /tmp/content/tmp/tensorboard/grpo --port=0"
      ],
      "metadata": {
        "id": "oVw87Cutnhcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ],
      "metadata": {
        "id": "lEcYDs-ZnsEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine=ROLLOUT_ENGINE,\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        eos_tokens=[tokenizer.encode(\"<|im_end|>\")[0]],\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=2,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        "    system_prompt=\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "oUcAc6ZSnuQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RL cluster\n",
        "with mesh:\n",
        "  rl_cluster = rl_cluster_lib.RLCluster(\n",
        "      actor=qwen2,\n",
        "      reference=qwen2,\n",
        "      tokenizer=tokenizer,\n",
        "      cluster_config=cluster_config,\n",
        "  )\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[math_rewards.math_reward,],\n",
        "    grpo_config=grpo_config,\n",
        "    chat_parser=chat_parser,\n",
        ")"
      ],
      "metadata": {
        "id": "jvKATTpunxgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_trainer.train(train_dataset)"
      ],
      "metadata": {
        "id": "CXdPz9A-nzCZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples:colab_kernel",
        "kind": "private"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
