{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsMzYTX_E0gR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  from GOOGLE_INTERNAL_PACKAGE_PATH.pyglib import gfile\n",
        "  file_open = gfile.Open\n",
        "  \n",
        "  NOTEBOOK_ENV = \"g3\"\n",
        "except Exception:\n",
        "  NOTEBOOK_ENV = \"git\"\n",
        "\n",
        "  from google.cloud import storage\n",
        "\n",
        "  client = storage.Client()\n",
        "  bucket = client.bucket(\"tunix\")\n",
        "  file_open = lambda path, mode=\"r\": bucket.blob(path).open(mode)\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "if NOTEBOOK_ENV == \"g3\":\n",
        "  DATA_PATH_PREFIX =  \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/\"\n",
        "  MODEL_PATH_PREFIX = \"/GOOGLE_INTERNAL_STOAGE_PATH/gg-d/home/qwix-dev/\"\n",
        "else:\n",
        "  DATA_PATH_PREFIX = \"gs://tunix/rl/data\"\n",
        "  MODEL_PATH_PREFIX = \"gs://tunix/rl/models\"\n",
        "\n",
        "DEEPSCALER_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"DeepScaleR-Preview-Dataset/deepscaler.json\")\n",
        "AIME_2024_DATA_PATH = os.path.join(DATA_PATH_PREFIX, \"HuggingFaceH4/aime_2024/train-00000-of-00001.parquet\")\n",
        "\n",
        "def create_datasets(\n",
        "    train_ds_path: str = DEEPSCALER_DATA_PATH,\n",
        "    test_ds_path: str = AIME_2024_DATA_PATH\n",
        "):\n",
        "  def preprocess_fn(example):\n",
        "    return {\n",
        "        \"question\": example[\"problem\"],\n",
        "        \"ground_truth\": example[\"answer\"],\n",
        "        \"data_source\": \"math\",\n",
        "    }\n",
        "\n",
        "  with file_open(train_ds_path) as train_f, file_open(test_ds_path, 'rb') as test_f:\n",
        "    train_df = pd.read_json(train_f)\n",
        "    test_df = pd.read_parquet(test_f)\n",
        "\n",
        "  train_ds = Dataset.from_pandas(train_df).map(preprocess_fn)\n",
        "  test_ds = Dataset.from_pandas(test_df).map(preprocess_fn)\n",
        "\n",
        "  return train_ds, test_ds\n",
        "\n",
        "train_ds, test_ds = create_datasets()\n",
        "\n",
        "for s in iter(train_ds):\n",
        "  print(s)\n",
        "  break\n",
        "\n",
        "for s in iter(test_ds):\n",
        "  print(s)\n",
        "  break\n"
      ]
    },
    {
      "metadata": {
        "id": "QNDch_ZJV3Xb"
      },
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "from flax import nnx\n",
        "import os\n",
        "\n",
        "try:\n",
        "  from etils import ecolab\n",
        "  cm = ecolab.adhoc(\n",
        "      source=ecolab.FROM_NOTEBOOK_OR_HEAD,\n",
        "      reload='tunix',\n",
        "      behavior='preferred',\n",
        "      cell_autoreload=True,\n",
        "  )\n",
        "except:\n",
        "  import contextlib\n",
        "  cm = contextlib.nullcontext()\n",
        "\n",
        "with cm:\n",
        "  from tunix.models.qwen2 import params as params_lib\n",
        "  from tunix.models.qwen2 import model as model_lib\n",
        "  from tunix.generate import sampler as sampler_lib\n",
        "\n",
        "MODEL_VERSION = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "MODEL_PATH = os.path.join(MODEL_PATH_PREFIX, \"/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "\n",
        "mesh = jax.make_mesh((1, 4), ('fsdp', 'tp'))\n",
        "config = model_lib.ModelConfig.deepseek_r1_distill_qwen_1_5b()\n",
        "model = params_lib.create_model_from_safe_tensors(MODEL_PATH, config, mesh)\n",
        "# nnx.display(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_source = MODEL_PATH if NOTEBOOK_ENV == \"g3\" else MODEL_VERSION\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sampler_lib.CacheConfig(\n",
        "        cache_size=8192, num_layers=model.config.num_layers, num_kv_heads=model.config.num_kv_heads, head_dim=model.config.head_dim\n",
        "    ),\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "tsM5VlCqZYsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "q = next(iter(test_ds.select(range(1))))['question']\n",
        "\n",
        "INSTRUCTION = \"Let's think step by step, and put your final answer within \\\\boxed{}.\"\n",
        "PROMPT = f\"{q} {INSTRUCTION}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": PROMPT},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=False,\n",
        "\treturn_dict=False,\n",
        ")\n",
        "print(inputs)\n",
        "\n",
        "out = sampler([inputs], max_generation_steps=1024, echo=True)\n",
        "pprint(out.text)"
      ],
      "metadata": {
        "id": "V51_flBNdgkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"Let's think step by step, and put your final answer within \\\\boxed{}.\"\n",
        "\n",
        "TEMPLATE = \"\"\"{question}{system_prompt}\"\"\"\n",
        "\n",
        "def templatize(prompts, tokenizer):\n",
        "  out = []\n",
        "  for p in prompts:\n",
        "    out.append(\n",
        "\n",
        "        tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": p},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True,\n",
        "        )\n",
        "    )\n",
        "  return out\n",
        "\n",
        "def generate(\n",
        "    question, tokenizer, sampler, max_generation_steps, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "  if isinstance(question, str):\n",
        "    question = [question]\n",
        "  input_batch = templatize(question, tokenizer)\n",
        "\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_generation_steps=max_generation_steps,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=True,\n",
        "      seed=seed if seed is not None else None,\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if isinstance(question, str):\n",
        "    return output[0]\n",
        "  return output"
      ],
      "metadata": {
        "id": "A8DTWo27dPA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "\n",
        "match_numbers = re.compile(\n",
        "    rf\"\\\\boxed.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "def last_boxed_only_string(string):\n",
        "    idx = string.rfind(\"\\\\boxed\")\n",
        "    if idx < 0:\n",
        "        idx = string.rfind(\"\\\\fbox\")\n",
        "        if idx < 0:\n",
        "            return None\n",
        "\n",
        "    i = idx\n",
        "    right_brace_idx = None\n",
        "    num_left_braces_open = 0\n",
        "    while i < len(string):\n",
        "        if string[i] == \"{\":\n",
        "            num_left_braces_open += 1\n",
        "        if string[i] == \"}\":\n",
        "            num_left_braces_open -= 1\n",
        "            if num_left_braces_open == 0:\n",
        "                right_brace_idx = i\n",
        "                break\n",
        "        i += 1\n",
        "\n",
        "    if right_brace_idx is None:\n",
        "        retval = None\n",
        "    else:\n",
        "        retval = string[idx : right_brace_idx + 1]\n",
        "\n",
        "    return retval\n",
        "\n",
        "\n",
        "def remove_boxed(s):\n",
        "    left = \"\\\\boxed{\"\n",
        "    try:\n",
        "        assert s[: len(left)] == left\n",
        "        assert s[-1] == \"}\"\n",
        "        return s[len(left) : -1]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_boxed_answer(solution: str) -> str:\n",
        "    \"\"\"Extract the answer from inside a LaTeX \\\\boxed{} command\"\"\"\n",
        "    solution = last_boxed_only_string(solution)\n",
        "    solution = remove_boxed(solution)\n",
        "    return solution\n",
        "\n",
        "def extract_answer(passage: str) -> str:\n",
        "    if \"\\\\boxed\" in passage:\n",
        "        return extract_boxed_answer(passage)\n",
        "    return None\n",
        "\n",
        "match_answer = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"\\\\boxed\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    tokenizer,\n",
        "    max_generation_steps=4096,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, tokenizer, sampler, max_generation_steps, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      print(responses)\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = extract_answer(response)\n",
        "        if extracted_response is None:\n",
        "          extracted_response = \"-1000000\"\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check answer generated\n",
        "        if match_answer.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ],
      "metadata": {
        "id": "EVLDIiw8daCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "sampler = sampler_lib.Sampler(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sampler_lib.CacheConfig(\n",
        "        cache_size=32768 + 2048, num_layers=model.config.num_layers, num_kv_heads=model.config.num_kv_heads, head_dim=model.config.head_dim\n",
        "    ),\n",
        ")\n",
        "\n",
        "evaluate(test_ds, sampler, tokenizer, max_generation_steps=32768)"
      ],
      "metadata": {
        "id": "RQGslQryeevc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LE_GLmcXhD2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples:colab_kernel",
        "kind": "private"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
